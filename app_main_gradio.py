# T√™n file: app_gradio.py
# (File n√†y thay th·∫ø cho app(2).py v√† pipeline_orchestrator.py)

import gradio as gr
import pandas as pd
import os
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    BitsAndBytesConfig,
    AutoModelForSequenceClassification,
    pipeline
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from typing import List, Dict, Any

# =L·ªói Import: Kh√¥ng t√¨m th·∫•y file th∆∞ vi·ªán. H√£y ƒë·∫£m b·∫£o c√°c file 'split_clause_lib.py', 'Term_Opinion_lib.py', 'Extract_Category.py', 'Extract_Polarity_lib.py' n·∫±m c√πng th∆∞ m·ª•c.
# =============================================================================
# B∆Ø·ªöC 1: IMPORT C√ÅC H√ÄM "WORKER" T·ª™ C√ÅC FILE LIB
# =============================================================================
try:
    # Import c√°c h√†m worker (ch·ªâ ch·ª©a logic, kh√¥ng t·∫£i model)
    from split_clause_lib import run_split_and_term, chat_llm
    from Term_Opinion_lib import run_extract_opinions
    from Extract_Category import extract_categories
    from Extract_Polarity_lib import run_detect_polarity
except ImportError as e:
    print(f"L·ªói Import: {e}")
    print("H√£y ƒë·∫£m b·∫£o c√°c file _lib.py n·∫±m c√πng th∆∞ m·ª•c.")
    exit()

# =============================================================================
# B∆Ø·ªöC 2: T·∫¢I T·∫§T C·∫¢ M√î H√åNH (T·∫¢I TO√ÄN C·ª§C - CH·ªà 1 L·∫¶N)
# (ƒê√¢y l√† logic t·ª´ pipeline_orchestrator.py, nh∆∞ng ƒë√£ lo·∫°i b·ªè
#  t·∫•t c·∫£ code c·ªßa Streamlit)
# =============================================================================

def load_qwen_model():
    print("ƒêang t·∫£i m√¥ h√¨nh Qwen LLM (4-bit)...")
    model_id = "Qwen/Qwen3-4B-Instruct-2507"
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    ) #
    
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        device_map="auto"
    )
    
    model = prepare_model_for_kbit_training(model)
    lora_config = LoraConfig(
        r=64, lora_alpha=16, target_modules=["q_proj", "v_proj"],
        lora_dropout=0.1, bias="none", task_type="CAUSAL_LM"
    ) #
    model = get_peft_model(model, lora_config)
    print("T·∫£i xong m√¥ h√¨nh Qwen LLM.")
    return model, tokenizer

def load_category_model():
    """T·∫£i m√¥ h√¨nh RoBERTa ƒë√£ fine-tune cho (Category)."""
    print("ƒêang t·∫£i m√¥ h√¨nh Category (RoBERTa)...")
    try:
        base_dir = os.path.dirname(os.path.abspath(__file__))
        model_path = os.path.join(base_dir, "roberta_lora_goal")
        
        model_cat = AutoModelForSequenceClassification.from_pretrained(model_path)
        tokenizer_cat = AutoTokenizer.from_pretrained(model_path)
        
        id2label = {
            0: "Amenity", 1: "Branding", 2: "Experience",
            3: "Facility", 4: "Loyalty", 5: "Service"
        } #
        print("T·∫£i xong m√¥ h√¨nh Category.")
        return model_cat, tokenizer_cat, id2label
    except Exception as e:
        print(f"L·ªói: Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh Category t·ª´ '{model_path}'.")
        print(f"B·∫°n ƒë√£ ch·∫°y script 'Fine_Tune_RoBertaBase.py' ch∆∞a? L·ªói chi ti·∫øt: {e}")
        return None, None, None

def load_polarity_model():
    """T·∫£i m√¥ h√¨nh DeBERTa cho (Polarity)."""
    print("ƒêang t·∫£i m√¥ h√¨nh Polarity (DeBERTa)...")
    polarity_classifier = pipeline(
        "text-classification",
        model="yangheng/deberta-v3-base-absa-v1.1",
        top_k=None, truncation=True,
        device=0 if torch.cuda.is_available() else -1
    ) #
    print("T·∫£i xong m√¥ h√¨nh Polarity.")
    return polarity_classifier

# --- T·∫¢I M√î H√åNH NGAY KHI APP KH·ªûI ƒê·ªòNG ---
MODELS = {}
try:
    qwen_model, qwen_tokenizer = load_qwen_model()
    cat_model, cat_tokenizer, cat_id2label = load_category_model()
    polarity_classifier = load_polarity_model()
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if cat_model:
        cat_model.to(device).eval()
        
    MODELS = {
        "qwen_model": qwen_model,
        "qwen_tokenizer": qwen_tokenizer,
        "cat_model": cat_model,
        "cat_tokenizer": cat_tokenizer,
        "cat_id2label": cat_id2label,
        "polarity_classifier": polarity_classifier,
        "device": device
    }
    print("--- T·∫§T C·∫¢ M√î H√åNH ƒê√É S·∫¥N S√ÄNG ---")
except Exception as e:
    print(f"L·ªói nghi√™m tr·ªçng khi t·∫£i m√¥ h√¨nh: {e}")
    MODELS = None # ƒê√°nh d·∫•u l√† t·∫£i l·ªói

# =============================================================================
# B∆Ø·ªöC 3: H√ÄM PIPELINE (ƒê∆Ø·ª¢C GRADIO G·ªåI M·ªñI L·∫¶N CLICK)
# =============================================================================

def run_pipeline_gradio(sentence: str, progress=gr.Progress(track_tqdm=True)):
    """
    H√†m n√†y ƒë∆∞·ª£c Gradio g·ªçi ƒë·ªÉ ch·∫°y to√†n b·ªô pipeline 5 b∆∞·ªõc.
    N√≥ s·ª≠ d·ª•ng c√°c m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i ·ªü B∆Ø·ªöC 2.
    """
    if not MODELS or not all(MODELS.values()):
        raise gr.Error("M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng. Vui l√≤ng ki·ªÉm tra console.")
    
    try:
        # Tr√≠ch xu·∫•t c√°c m√¥ h√¨nh t·ª´ bi·∫øn to√†n c·ª•c
        qwen_model = MODELS["qwen_model"]
        qwen_tokenizer = MODELS["qwen_tokenizer"]
        cat_model = MODELS["cat_model"]
        cat_tokenizer = MODELS["cat_tokenizer"]
        cat_id2label = MODELS["cat_id2label"]
        polarity_classifier = MODELS["polarity_classifier"]
        device = MODELS["device"]

        # T·∫°o h√†m chat_func (logic t·ª´ app(2).py)
        def chat_func(messages, max_new_tokens):
            return chat_llm(qwen_model, qwen_tokenizer, messages, max_new_tokens)

        # Ch·∫°y 4 b∆∞·ªõc pipeline
        progress(0.25, desc="B∆∞·ªõc 1/4: T√°ch Clause v√† Term (Qwen LLM)...")
        clauses_terms = run_split_and_term(sentence, chat_func)
        
        progress(0.50, desc="B∆∞·ªõc 2/4: Tr√≠ch xu·∫•t Opinion (Qwen LLM)...")
        clauses_opinions = run_extract_opinions(clauses_terms, chat_func)
        
        progress(0.75, desc="B∆∞·ªõc 3/4: D·ª± ƒëo√°n Category (RoBERTa)...")
        clauses_categories = extract_categories(
            clauses_opinions, cat_model, cat_tokenizer, cat_id2label, device
        )
        
        progress(0.90, desc="B∆∞·ªõc 4/4: Ph√°t hi·ªán Polarity (DeBERTa)...")
        final_results = run_detect_polarity(clauses_categories, polarity_classifier)
        
        # ƒê·ªãnh d·∫°ng k·∫øt qu·∫£
        df = pd.DataFrame(final_results)
        columns_order = [
            "clause", "term", "opinion", "category", "polarity", 
            "polarity_score", "sentence_original"
        ]
        final_columns = [col for col in columns_order if col in df.columns]
        return df[final_columns]
    
    except Exception as e:
        print(f"L·ªói pipeline: {e}")
        raise gr.Error(f"ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh x·ª≠ l√Ω: {e}")

# =============================================================================
# B∆Ø·ªöC 4: X√ÇY D·ª∞NG GIAO DI·ªÜN GRADIO
# =============================================================================

# L·∫•y v√≠ d·ª• t·ª´ file Colab c·ªßa b·∫°n
example_sentences = [
    "The food was great and the staff was very friendly, but the room was a bit small.",
    "Quy tr√¨nh check-in r·∫•t su√¥n s·∫ª v√† nh√¢n vi√™n v√¥ c√πng nhi·ªát t√¨nh. Ph√≤ng ·ªëc c·ªßa ch√∫ng t√¥i r·ªông r√£i v√† chi·∫øc gi∆∞·ªùng r·∫•t tho·∫£i m√°i, m·∫∑c d√π wifi trong ph√≤ng l·∫°i ch·∫≠m m·ªôt c√°ch kh·ªßng khi·∫øp. Ch√∫ng t√¥i r·∫•t th√≠ch h·ªì b∆°i, nh∆∞ng thi·∫øt b·ªã ph√≤ng gym c√≥ v·∫ª ƒë√£ kh√° c≈© v√† l·ªói th·ªùi. D√π l√† th√†nh vi√™n th√¢n thi·∫øt, ch√∫ng t√¥i h∆°i th·∫•t v·ªçng v√¨ kh√¥ng ƒë∆∞·ª£c n√¢ng h·∫°ng ph√≤ng, nh∆∞ng nh√¨n chung tr·∫£i nghi·ªám t·ªïng th·ªÉ r·∫•t tuy·ªát v·ªùi v√† ch·∫Øc ch·∫Øn ch√∫ng t√¥i s·∫Ω quay l·∫°i.",
    "Nh√† h√†ng trong kh√°ch s·∫°n c√≥ ƒë·ªì ƒÉn r·∫•t ngon, nh∆∞ng d·ªãch v·ª• t·∫°i b√†n l·∫°i ch·∫≠m. Qu·∫ßy bar tr√™n t·∫ßng th∆∞·ª£ng c√≥ t·∫ßm nh√¨n tuy·ªát ƒë·∫πp, ƒë√≥ th·ª±c s·ª± l√† m·ªôt tr·∫£i nghi·ªám ƒë√°ng nh·ªõ. Tuy nhi√™n, khu v·ª±c s·∫£nh ch·ªù (lobby) l·∫°i kh√° ·ªìn √†o. Th∆∞∆°ng hi·ªáu n√†y lu√¥n l√†m t√¥i h√†i l√≤ng v·ªÅ s·ª± s·∫°ch s·∫Ω."
]

with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # üöÄ Pipeline Ph√¢n t√≠ch C·∫£m x√∫c (ABSA)
        Nh·∫≠p m·ªôt c√¢u ƒë√°nh gi√° (review) v√† ·ª©ng d·ª•ng s·∫Ω ch·∫°y pipeline 5 b∆∞·ªõc ƒë·ªÉ tr√≠ch xu·∫•t: **Clause, Term, Opinion, Category, Polarity**.
        """
    )
    
    with gr.Column():
        text_input = gr.Textbox(
            lines=5,
            label="Nh·∫≠p c√¢u ƒë√°nh gi√° c·ªßa b·∫°n:",
            placeholder="V√≠ d·ª•: The food was great and the staff was very friendly..."
        )
        analyze_button = gr.Button("Ph√¢n t√≠ch (Run Pipeline)", variant="primary")
        
        gr.Markdown("---")
        
        output_df = gr.DataFrame(
            label="K·∫øt qu·∫£ Ph√¢n t√≠ch Pipeline",
            wrap=True
        )
        
        gr.Examples(
            examples=example_sentences,
            inputs=text_input
        )
        
    # Li√™n k·∫øt n√∫t b·∫•m v·ªõi h√†m x·ª≠ l√Ω
    analyze_button.click(
        fn=run_pipeline_gradio,
        inputs=text_input,
        outputs=output_df
    )

# Ch·∫°y ·ª©ng d·ª•ng
if __name__ == "__main__":
    demo.queue().launch(share=True)